{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBlRB9mcfrAR3D3HmDUyyc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliazhu09/TextAnalysis/blob/main/Text_Analysis_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python for Text Analysis\n",
        "\n",
        "### Xiaojuan Zhu\n",
        "### OIT Research Computing Support\n",
        "### Help Desk: 865-974-9900 "
      ],
      "metadata": {
        "id": "J5uvwYsZrTGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use scikit-learn (version 1.0.2) and nltk package to clean the text data and fit the topic modeling for today. Let us check and install these packages. "
      ],
      "metadata": {
        "id": "mAkuq68bb7e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "id": "A_tZA1tKilAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the scikit-learn package installed colab is not 1.0.2, we need to uninstall it and reinstall the scikite-learn of version 1.0.2."
      ],
      "metadata": {
        "id": "lQsM2iJNeInG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==1.0.2"
      ],
      "metadata": {
        "id": "AzOux4XBYnx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK is natural language toolkit. It provides easy-to-use interfaces to over 50 corpora and lexical resources. NLTK package has already been installed in Colab, we just need to import it and download some corpora we need, such as stopwords."
      ],
      "metadata": {
        "id": "oJ_5Mshregwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import numpy as np \n"
      ],
      "metadata": {
        "id": "eiKnvnLrWZZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we used for today's workshop is WallStreetbets posts data. WallStreetBets (r/wallstreetbets, also known as WSB), is a subreddit where participants discuss stock and option trading. It has become notable for its profane nature and allegations of users manipulating securities.\n",
        "\n",
        "The community became mainstream again with its interest on GameStop shares in 2021.\n",
        "\n",
        "We want to figure out why and how it happens and we'll try to to find what are the main topics in the body of the wallstreetbets posts.\n",
        "Let us import the data. "
      ],
      "metadata": {
        "id": "ckBHwY6HDFJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# reading in the dataset\n",
        "url = \"https://drive.google.com/uc?id=17H5Y6EEQ-80VOwfYViW8GSB1w-QUOGDD&authuser=xzhu8%40vols.utk.edu&usp=drive_fs\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "KMIrC-173qJj",
        "outputId": "5a55556c-2560-40c5-99d5-e7bdbfbba7db"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  score      id  \\\n",
              "0  It's not about the money, it's about sending a...     55  l6ulcx   \n",
              "1  Math Professor Scott Steiner says the numbers ...    110  l6uibd   \n",
              "2                                    Exit the system      0  l6uhhn   \n",
              "3  NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...     29  l6ugk6   \n",
              "4  Not to distract from GME, just thought our AMC...     71  l6ufgy   \n",
              "\n",
              "                                                 url  comms_num       created  \\\n",
              "0                    https://v.redd.it/6j75regs72e61          6  1.611863e+09   \n",
              "1                    https://v.redd.it/ah50lyny62e61         23  1.611862e+09   \n",
              "2  https://www.reddit.com/r/wallstreetbets/commen...         47  1.611862e+09   \n",
              "3  https://sec.report/Document/0001193125-21-019848/         74  1.611862e+09   \n",
              "4                https://i.redd.it/4h2sukb662e61.jpg        156  1.611862e+09   \n",
              "\n",
              "                                                body            timestamp  \n",
              "0                                                NaN  2021-01-28 21:37:41  \n",
              "1                                                NaN  2021-01-28 21:32:10  \n",
              "2  The CEO of NASDAQ pushed to halt trading “to g...  2021-01-28 21:30:35  \n",
              "3                                                NaN  2021-01-28 21:28:57  \n",
              "4                                                NaN  2021-01-28 21:26:56  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae2e1355-945b-4507-a450-a39d8ce3c124\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>score</th>\n",
              "      <th>id</th>\n",
              "      <th>url</th>\n",
              "      <th>comms_num</th>\n",
              "      <th>created</th>\n",
              "      <th>body</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It's not about the money, it's about sending a...</td>\n",
              "      <td>55</td>\n",
              "      <td>l6ulcx</td>\n",
              "      <td>https://v.redd.it/6j75regs72e61</td>\n",
              "      <td>6</td>\n",
              "      <td>1.611863e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-01-28 21:37:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
              "      <td>110</td>\n",
              "      <td>l6uibd</td>\n",
              "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
              "      <td>23</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-01-28 21:32:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Exit the system</td>\n",
              "      <td>0</td>\n",
              "      <td>l6uhhn</td>\n",
              "      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n",
              "      <td>47</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>The CEO of NASDAQ pushed to halt trading “to g...</td>\n",
              "      <td>2021-01-28 21:30:35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NEW SEC FILING FOR GME! CAN SOMEONE LESS RETAR...</td>\n",
              "      <td>29</td>\n",
              "      <td>l6ugk6</td>\n",
              "      <td>https://sec.report/Document/0001193125-21-019848/</td>\n",
              "      <td>74</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-01-28 21:28:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Not to distract from GME, just thought our AMC...</td>\n",
              "      <td>71</td>\n",
              "      <td>l6ufgy</td>\n",
              "      <td>https://i.redd.it/4h2sukb662e61.jpg</td>\n",
              "      <td>156</td>\n",
              "      <td>1.611862e+09</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2021-01-28 21:26:56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae2e1355-945b-4507-a450-a39d8ce3c124')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae2e1355-945b-4507-a450-a39d8ce3c124 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae2e1355-945b-4507-a450-a39d8ce3c124');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape function tell us how many columns and rows in the dataset. There are total 8 columns (variables). "
      ],
      "metadata": {
        "id": "Lo2p54fy9TBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "PzqwTaNPaY7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let's drop the missing values in the \"body\" column."
      ],
      "metadata": {
        "id": "VtKH_BuCZ6Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['body'], inplace=True)\n",
        "df['original_body'] = df['body']\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBlugcTZZ54J",
        "outputId": "db983333-ef79-4041-8685-5b086eea7912"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22521, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Cleaning & Preprocessing\n",
        "\n",
        "One of the most crucial phases when dealing with unstructured data such as text is the cleaning/preprocessing step. Sometimes this process is even more important than the model-building part.\n",
        "\n",
        "It is better to remove words that don't carry much information about the post itself such as punctuation, stop words, and others...\n",
        "\n",
        "In this section, I'm going to clean the body of the DataFrame preparing it for the successive phases.\n",
        "\n",
        "The cleaning steps that I'm going to apply are:\n",
        "\n",
        "* Removal of URLs\n",
        "* Removal of punctuation\n",
        "* Tokenization\n",
        "* Removal of stopwords\n",
        "* Lemmatization\n",
        "* Removal of other non-meaningful characters"
      ],
      "metadata": {
        "id": "C6tR0HJDWvnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Urls\n",
        "First, we need to remove the url, like the string starts with \"https\" or \"www\". We import **re** package and define a function to remove urls. In the function we use the complie function to tell python the string starting with https and www are the url string and needs to be deleted. Then we use sub function to remove those strings. "
      ],
      "metadata": {
        "id": "84K1I9ifKeh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "  "
      ],
      "metadata": {
        "id": "EHqYmafra8eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply() function simplifies applying a function on each element in a pandas series and each row or column in a pandas DataFrame. We use apply function to apply the remove_urls function to the body column and it will remove the urls from each row. \n"
      ],
      "metadata": {
        "id": "FVaDMqdwTkvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['body'] = df['body'].apply(remove_urls)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "5j_LCjW2hi6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove punctuation\n",
        "\n",
        "We're going to remove punctuation. In order to remove punctuation, we have to actually have a way to show Python what punctuation even looks like. Here, the string package has a list of punctuation in it. So, I'll import string, and then all we need to do is just call string.punctuation, and we can run that, and that'll print out a list of different punctuation. So, this is really helpful to allow Python to identify what we're looking for here. \n",
        "\n",
        "We care about this is because periods, parentheses, and other punctuation look like just another character to Python. But realistically, the period doesn't really help pull the meaning out of a sentence. For instance, for us \"Dog eats bones.\", with a period, is exactly the same as, \"Dog eats bones\" without a period. They mean the same thing to us, but when you give that to Python, Python says those are not equivalent things. And Python isn't saying \"Dog eats bones\" without a period is different from \"Dog eats bones\" with a period, in that they're really close, but one has a period and one doesn't. "
      ],
      "metadata": {
        "id": "jhumZWJ5ixrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "curfsYwpZtZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Dog eats bones.\" == \"Dog eats bones\"\n"
      ],
      "metadata": {
        "id": "4mZ8dcVBUgRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(text):\n",
        "   text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
        "   return text_nopunct\n",
        "df['body'] = df['body'].apply(remove_punct)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "_f2Ha_mff0la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1\n",
        "Remove the punctucation from the title column by using the remove_punct function"
      ],
      "metadata": {
        "id": "2IBmt4qphrs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1 \n",
        "df['title_nopunct'] = \n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZSCoUz3ahnVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization \n",
        "Tokenizing is splitting some string or sentence into a list of words. \n",
        "The \\W+, indicates that it will split wherever it sees one or more non-word characters. So that'll split on white spaces, special characters, anything like that. "
      ],
      "metadata": {
        "id": "1GM0lqS_cvjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    return tokens\n",
        "\n",
        "df['body_text_tokenized'] = df['body'].apply(lambda x:tokenize(x.lower())) # make the token lower cases\n"
      ],
      "metadata": {
        "id": "S4TjvXKvctE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python is case-sensitive, so if you give it capital NLP, and ask if its the same as lowercase nlp, we know to us, that's basically the same thing, but if you run that in Python, Python says those are two different things. So when we get in to further steps, now Python is seeing capital NLP is something totally different from lowercase nlp, and it has to learn that those things are closely related, but we don't want it to consume its resources by learning that those things are related, we want it to explicitly tell it that those are the same things. So that's why we apply this .lower method"
      ],
      "metadata": {
        "id": "UtVUsN0Akwm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"NLP\" == \"nlp\""
      ],
      "metadata": {
        "id": "h2sbYcCZkKew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QFHsFi3v6Hxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2\n",
        "Tokenize the title_nopunct column by apply the tokenize function and apply the lower case function simultaneously. "
      ],
      "metadata": {
        "id": "BjwfuQt9jRmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "df['title_text_tokenized'] = \n",
        "df.head()"
      ],
      "metadata": {
        "id": "85XHC1N-jcDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove stop words\n",
        "We want to remove some of the more irrelevant words in the lists. The stop words are commonly-used words like the, but, if, that don't contribute much to the meaning of a sentence. So we want to remove them, to limit the number of tokens Python actually has to look at when building our model. \n",
        "\n",
        "For example: **\"I am learning NLP\"**. After tokenizing, it would have four tokens, **I**, **am**, **learning**, and **NLP**. After removing stopwords, instead of a list with four tokens, you're now left with just **learning** and **NLP**.\n",
        "\n",
        "We will call the stopwords from the nltk package and we want to use English stop words. \n",
        "\n",
        "Then we create a function to remove the stop words. \n"
      ],
      "metadata": {
        "id": "kxDUV_EVjTSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(tokenized_list):\n",
        "    # I want to return word for word in tokenized_list if that word is not in stopword. \n",
        "    text = [word for word in tokenized_list if word not in stopword]\n",
        "    return text\n",
        "\n",
        "df['body_text_nostop'] = df['body_text_tokenized'].apply(remove_stopwords)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "73e5NB0AmtL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 3\n",
        "remove the stop words from the title_text_tokenized column\n"
      ],
      "metadata": {
        "id": "UbqThpiYkn9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['title_text_nostop'] = \n",
        "df.head()"
      ],
      "metadata": {
        "id": "zI4H1hwrkxXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming and Lemmatization\n",
        "**Stemming**: the process of reducing inflected (or derived) words to their word stem or root. The process of stemming often means crudely chopping off the end of a word, to leave only the base. Stemming's goal is to reduce the number of words python has to look at or consider. \n",
        "For example, Stemming/stemmed will be reduced to stem. \n",
        "Electricity/electrical will be reduced to Electr. \n",
        "Connection/connected/connective will be chopped down to connect. \n",
        "\n",
        "Stemmers are corect in most cases, but the tradeoff with these simple rules is that won't always be right. For example, geesee and goose are two separate things after stemming, that means it has to keep those two separate words in memory.  So we need to use lemmatization. \n",
        "\n"
      ],
      "metadata": {
        "id": "7W-kRJ62mMPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "wn =  nltk.WordNetLemmatizer()\n",
        "ps =  nltk.PorterStemmer()\n",
        "print(ps.stem('goose'))\n",
        "print(ps.stem('geese'))"
      ],
      "metadata": {
        "id": "YQuUB7STHwD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**:  the process of grouping together the inflected forms of a word so they can be analyzed as a single term, identified by the word's lemma. The lemma is the canonical form of a set of words. For example, goose and geese, it will return goose. \n",
        "\n",
        "Lemmatizing is using vocabulary analysis of words to remove inflectional endings and return to the dictionary form of a word.\n",
        "\n",
        "In practical, there's an accuracy and speed trade-off that you're making when you opt for one over the other. "
      ],
      "metadata": {
        "id": "hnx9iyJ9Hvqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(wn.lemmatize('goose'))\n",
        "print(wn.lemmatize('geese'))"
      ],
      "metadata": {
        "id": "L05XcgS_m1p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(tokenized_text):\n",
        "    text = [ps.stem(word) for word in tokenized_text]\n",
        "    return text\n",
        "\n",
        "df['body_text_stemmed'] = df['body_text_nostop'].apply(lambda x:stem(x))"
      ],
      "metadata": {
        "id": "hThz_1ESrt2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "6mZUiCycsD4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Exercise 4\n",
        "Apply stem function on the title_text_nostop column"
      ],
      "metadata": {
        "id": "Qb3TmxF1lFQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['title_text_stemmed'] = \n",
        "df.head()"
      ],
      "metadata": {
        "id": "wwpET_sQlJl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove other characters"
      ],
      "metadata": {
        "id": "-NCDdgoXFEFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "other_chars = ['*', '#', '&x200B', '[', ']', '; ',' ;' \"&nbsp\", \"“\",\"“\",\"”\", \"x200b\",\"_\", ',']\n",
        "\n",
        "def remove_other_chars(tokenized_list):\n",
        "  text = [word for word in tokenized_list if word not in other_chars]\n",
        "  return text\n",
        "\n",
        "df['body_text_nochars'] = df['body_text_stemmed'].apply(remove_other_chars)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BMosczJV1Pn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a clean text column to put the left words together, which suppose contain the words without irrelevant information. "
      ],
      "metadata": {
        "id": "bp8zy3jLBZV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['body_text_nochars'].apply(lambda x : ' '.join(x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "oDxnLRSv3a6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a body_list as the input of the topic modeling analysis in the final section. "
      ],
      "metadata": {
        "id": "u-O-ObXfCCki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "body_list = df.clean_text.tolist()\n",
        "body_list[:10]"
      ],
      "metadata": {
        "id": "mQHkLpqq-L7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Frequency\n",
        "\n"
      ],
      "metadata": {
        "id": "xJiMvPVMEQhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to create a list named comments which combines all the information from the clean_text column together and split/tokenize it by each word. I use tokenize here, but split function also works here. "
      ],
      "metadata": {
        "id": "NJ4t-7u_Sohk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments =\" \".join(df['clean_text'])\n",
        "words = tokenize(comments)\n",
        "words[:10]\n"
      ],
      "metadata": {
        "id": "AD5Kv2zCYRGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the Counter function from collections package to count the words. "
      ],
      "metadata": {
        "id": "bG64eAqU6kHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counted =Counter(ngrams(words,1)) \n",
        "data = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.barplot(x='frequency',y='word',data=data.head(30))\n"
      ],
      "metadata": {
        "id": "UVW-uPSvarH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you might guess in the top 5 we have the name of the stock that made r/wsb famous, we are talking about GME and the company name is GameStop Corp.\n",
        "\n",
        "Whereas if you look further in the top 40 you might encounter words like: stock, market, sell, share, company, ... which are all words related to the financial world.\n",
        "\n"
      ],
      "metadata": {
        "id": "hoJj0jP5_Z5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 5\n",
        "Count the word frequency of the title column and plot the first 20 words on the screen."
      ],
      "metadata": {
        "id": "ozyH4vlAm0Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_title'] = df['title_text_lemmatized'].apply(lambda x : ' '.join(x))\n",
        "title_comments = \" \".join(df['clean_title'])\n",
        "title_words = tokenize(title_comments)\n",
        "title_words[:10]\n",
        "\n",
        "### add the code here\n",
        "from collections import Counter\n",
        "counted =\n",
        "\n",
        "#### plot the figure\n",
        "data = pd.DataFrame(counted.items(),columns=['title_words','frequency']).sort_values(by='frequency',ascending=False)\n",
        "#plt.figure(figsize=(8, 10))\n",
        "sns.barplot(x='frequency',y='title_words',data=data.head(20))"
      ],
      "metadata": {
        "id": "vxk88RkVnJVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let us generate the word cloud. "
      ],
      "metadata": {
        "id": "0iC9fpI5mzCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS"
      ],
      "metadata": {
        "id": "-c_GyFmD_WpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', \n",
        "                          colormap='Set2', width=800, height=600\n",
        "                         ).generate(comments)\n",
        "\n",
        "plt.figure(figsize=(10, 7), frameon=True)\n",
        "plt.imshow(fig_wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QTDFMpNx_dbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully generated our first-word cloud image. It shows that most of the data talk about “STOCK”, Company”, “Gme”, “market”.\n",
        "\n",
        "#### Exercise 6\n",
        "Apply the word cloud to the title_comments."
      ],
      "metadata": {
        "id": "n5azJ7ROr0cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### file the code here\n",
        "fig_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', \n",
        "                          colormap='Set2', width=800, height=600\n",
        "                         ).generate()###code \n",
        "\n",
        "plt.figure(figsize=(10, 7), frameon=True)\n",
        "plt.imshow(fig_wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rL2eRMBKsIJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's now see what are the most common bigrams and trigrams in the dataset. Here I see gme and amc as a pair. \n",
        "\n",
        "n=2: bigram (pull out all combinations of two adjacent words in our text)\n",
        "\n",
        "n=3: trigrams (pull all combinations of three adjacent words in our text)"
      ],
      "metadata": {
        "id": "uEp-HGjVTD26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# top 2 pairs and top 3\n",
        "counted_2 =Counter(ngrams(words,2))\n",
        "counted_3= Counter(ngrams(words,3))\n",
        "word_pairs =pd.DataFrame(counted_2.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
        "trigrams =pd.DataFrame(counted_3.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
        "#word_pairs\n",
        "#plt.figure(figsize=(8, 25))\n",
        "#sns.barplot(y=counted_2, x=word_pairs)\n",
        "fig, axes = plt.subplots(2,1,figsize=(8,20))\n",
        "sns.barplot(ax=axes[0],x='frequency',y='pairs',data=word_pairs.head(30))\n",
        "sns.barplot(ax=axes[1],x='frequency',y='trigrams',data=trigrams.head(30))"
      ],
      "metadata": {
        "id": "VSCoUZQTQxrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic modelling\n",
        "\n",
        "Next, we are applying Non-Negative Matrix factorization (NMF) and Latent Dirichlet Allocation (LDA) on the comments and extract the topic structure of the corpus. The output is a plot of topics, each represented as bar plot using a top few words based on weights.\n"
      ],
      "metadata": {
        "id": "MiAqNArtOOCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Vectorization\n",
        "Before we apply NMF and LDA, we need to vectorize the text data. Vectorizing is a process that we use to convert text to a mathematical form that Python and a machine learning model can understand. It is the process of converting text into numerical representation.  \n",
        "\n",
        "Here are three methods to accomplish text vectorization: \n",
        "\n",
        "\n",
        "*   Count vectorization: \n",
        "*   N-grams\n",
        "*   Term frequency-inverse document frequency (TF-IDF)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tn_rxf57P1Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Count Vectorization\n",
        "Creates a document-term matrix and the entry of each cell will be a count of the number of times that word occurred in that document. \n",
        "\n",
        "Here is an example:"
      ],
      "metadata": {
        "id": "GnhfnWJ9V0Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [ \"I Have a Dream, a song to sing. To help me cope, with anything. If you see the wonder, of a fairy tale. You can take the future, even if you fail\",\n",
        "\"I believe in angels. Something good in everything I see. I believe in angels. When I know the time is right for me. I will cross the stream, I Have a Dream\"]"
      ],
      "metadata": {
        "id": "SPAfgRm3ON0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "Count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "X_count = Count_vectorizer.fit_transform(corpus)\n",
        "df =  pd.DataFrame(X_count.toarray(), columns = Count_vectorizer.get_feature_names())\n",
        "df"
      ],
      "metadata": {
        "id": "4kcbciPvWD9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Grams\n",
        "Creates a documents-term matrix where counts still occupy the cell but instead of the columns representing single term, they represent all combinations of adjacent words of length in the text.   \n",
        "\n",
        "n=2: bigram (pull out all combinations of two adjacent words in our text)\n",
        "\n",
        "n=3: trigrams (pull all combinations of three adjacent words in our text)\n",
        "\n",
        "Let us look at the example, n=2\n"
      ],
      "metadata": {
        "id": "aCel2z2CZASb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_vectorizer = CountVectorizer(ngram_range=(2,2), stop_words=\"english\")\n",
        " # ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, \n",
        " # and (2, 2) means only bigrams. Only applies if analyzer is not callable.\n",
        "ngram_count = ngram_vectorizer.fit_transform(corpus)\n",
        "df =  pd.DataFrame(ngram_count.toarray(), columns = ngram_vectorizer.get_feature_names())\n",
        "df"
      ],
      "metadata": {
        "id": "NNZ1L10UbMuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus[1].\n",
        "\n",
        "\n",
        "\n",
        "$$ W_{i,j} = tf_{i,j}  \\times log({N \\over df_i}) $$\n",
        "\n",
        "$tf_{i,j}=$ number of times i occurs in j divided by total number of terms in j\n",
        "\n",
        "$df_i=$ number of documents containing i\n",
        "\n",
        "$N =$ total number of documents.   \n",
        "\n",
        "For example, \n",
        "\"I like NLP\".\n",
        "\n",
        "$$ tf_{NLP,j} = {\\frac{\\text{# of occurences of NLP}}{\\text{number of words in the text}}} = 1/3 $$ \n",
        "\n",
        "There are total 20 text sentences and only 1 of those contains NLP.  \n",
        "\n",
        "$N = 20$ \n",
        "\n",
        "$df_{NLP} = 1 $\n",
        "\n",
        "$ w_{i,j} = tf_{i,j} \\times log({N \\over df_i})$\n",
        "\n",
        "$ w_{i,j} = 1/3 \\times log(20/1) = 0.43  $\n",
        "\n",
        "Now let's say that you have 40 text messages instead of 20, but NLP still only occurs in one of them. Now this fraction is 40 over 1. The term NLP is less frequent, and this term collectively is going to be larger. Basically, all this shows us is that the rarer the word is, the higher that this value's going to be. In summary, this method helps us pull out important but seldom-used words. \n",
        "\n",
        "Reference: [1].Rajaraman, A.; Ullman, J.D. (2011). \"Data Mining\" (PDF). Mining of Massive Datasets. pp. 1–17. doi:10.1017/CBO9781139058452.002. ISBN 978-1-139-05845-2.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c93Ki2gOcBpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "tf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X_tfidf = tf_vectorizer.fit_transform(corpus)\n",
        "df =  pd.DataFrame(X_tfidf.toarray(), columns = tf_vectorizer.get_feature_names())\n",
        "df"
      ],
      "metadata": {
        "id": "tHcKibiNPdbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What is Topic Modeling?\n",
        "\n",
        "Topic modeling is an unsupervised technique that intends to analyze large volumes of text data by clustering the documents into groups. In the case of topic modeling, the text data do not have any labels attached to it. Rather, topic modeling tries to group the documents into clusters based on similar characteristics.\n",
        "\n",
        "A typical example of topic modeling is clustering a large number of newspaper articles that belong to the same category. In other words, cluster documents that have the same topic. It is important to mention here that it is extremely difficult to evaluate the performance of topic modeling since there are no right answers. It depends upon the user to find similar characteristics between the documents of one cluster and assign it an appropriate label or topic.\n",
        "\n",
        "Two approaches are mainly used for topic modeling: **Non-Negative Matrix factorization** and **Latent Dirichlet Allocation**. Next, we will briefly review both of these approaches and will see how they can be applied to topic modeling in Python.\n",
        "\n",
        "**Non-Negative Matrix Factorization (NMF)**\n",
        "\n",
        "Non-negative matrix factorization is also an unsupervised learning technique which performs clustering as well as dimensionality reduction. In text mining consider the bag-of-words matrix representation where each row corresponds to a document, and each column to a word.\n",
        "\n",
        "NMF will produce two matrices W and H. The columns of W can be interpreted as basis documents (bags of words). What interpretation can we give to such a basis document in this case? They represent topics! Sets of words found simultaneously in different documents. H tells us how to sum contributions from different topics to reconstruct the word mix of a given original document.\n",
        "<img src=\"https://drive.google.com/uc?id=17eF4KRsqdEQjdBkiF4b87_NchASCh0qE&authuser=xzhu8%40vols.utk.edu&usp=drive_fs\" width=\"800\" height=\"600\" />\n",
        "\n",
        "\n",
        "Therefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics.\n",
        "\n",
        "Reference: https://blog.acolyer.org/2019/02/18/the-why-and-how-of-nonnegative-matrix-factorization/\n",
        "\n"
      ],
      "metadata": {
        "id": "e5KY1XpOWkSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20"
      ],
      "metadata": {
        "id": "nZyqDMafQKwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a topic plot function"
      ],
      "metadata": {
        "id": "-uzFaspdXGgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
        "        for i in \"top right left\".split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "p89LNrYpPuTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting tf-idf features for NMF"
      ],
      "metadata": {
        "id": "85OSy7xYQP6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        ") # max_df = 0.95 means \"ignore terms that appear in more than 95% of the documents\"\n",
        "  # min_df = 2 means \"ignore terms that appear in less than 2 documents\". \n",
        "tfidf = tfidf_vectorizer.fit_transform(body_list)"
      ],
      "metadata": {
        "id": "53BO5zW-QQ_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the NMF model"
      ],
      "metadata": {
        "id": "FY3rCwSZSjzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf = NMF(n_components=n_components, random_state=1).fit(tfidf)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(\n",
        "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
        ")"
      ],
      "metadata": {
        "id": "BVrpzY4fSlvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the NMF model (generalized Kullback-Leibler divergence).\n",
        "\n",
        "The Kullback–Leibler divergence (also called relative entropy and I-divergence), is a type of statistical distance: a measure of how one probability distribution P is different from a second reference probability distribution Q. The unnormalized generalization of Kullback-Leibler (KL) divergence is commonly used in Nonnegative Matrix Factorization (NMF). "
      ],
      "metadata": {
        "id": "cC0pT1iWTJqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf = NMF(\n",
        "    n_components=n_components,\n",
        "    random_state=1,\n",
        "    beta_loss=\"kullback-leibler\",\n",
        "    solver=\"mu\", # \n",
        "    max_iter=1000,\n",
        "   ).fit(tfidf)\n",
        "\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(\n",
        "    nmf,\n",
        "    tfidf_feature_names,\n",
        "    n_top_words,\n",
        "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
        ")"
      ],
      "metadata": {
        "id": "82vRlQ-zTKBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Latent Dirichlet Allocation (LDA)**\n",
        "\n",
        "The LDA is based upon two general assumptions:\n",
        "Documents that have similar words usually have the same topic.\n",
        "Documents that have groups of words frequently occurring together usually have the same topic.\n",
        "These assumptions make sense because the documents that have the same topic, for instance, Business topics will have words like the \"economy\", \"profit\", \"the stock market\", \"loss\", etc. The second assumption states that if these words frequently occur together in multiple documents, those documents may belong to the same category.\n",
        "\n",
        "Mathematically, the above two assumptions can be represented as:\n",
        "\n",
        "* Documents are probability distributions over latent topics\n",
        "* Topics are probability distributions over words\n",
        "\n",
        "![image](https://drive.google.com/uc?id=17eJ1yYu2UlBqPbQWDQTDtfMakTgIYO9e&authuser=xzhu8%40vols.utk.edu&usp=drive_fs)\n",
        "\n",
        "For example: suppose we have the following set of sentences:\n",
        "\n",
        "\n",
        "*   I eat fish and vegetables\n",
        "*   Fish are pets\n",
        "*   My kitten eats fish \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=17kv2RkTxRuFOGkvyflK-PNhRoH2E5AMY&authuser=xzhu8%40vols.utk.edu&usp=drive_fs\" width=\"300\" height=\"200\" />\n",
        "\n",
        "\n",
        "Given the above sentences, LDA might classify the red words under the Topic F, which we might label as “food“. Similarly, blue words might be classified under a separate Topic P, which we might label as “pets“. LDA defines each topic as a bag of words, and you have to label the topics as you deem fit.\n",
        "\n",
        "There are 2 benefits from LDA defining topics on a word-level:\n",
        "\n",
        "1) We can infer the content spread of each sentence by a word count:\n",
        "\n",
        "> Sentence 1: 100% Topic F\n",
        "\n",
        "> Sentence 2: 100% Topic P\n",
        "\n",
        "> Sentence 3: 33% Topic P and 67% Topic F\n",
        "\n",
        "2) We can derive the proportions that each word constitutes in given topics. \n",
        "\n",
        "For example, Topic F might comprise words in the following proportions: 40% eat (2), 40% fish(2), 20% vegetables(1).\n",
        "\n",
        "LDA achieves the above results in 3 steps.\n",
        "\n",
        "To illustrate these steps, imagine that we are now discovering topics in documents instead of sentences. \n",
        "\n",
        "**Step 1**: You tell the algorithm how many topics you think there are.\n",
        "\n",
        "**Step 2**: The algorithm will assign every word to a temporary topic. \n",
        "\n",
        "**Step 3**: The algorithm will check and update topic assignments, looping through each word in every document. \n",
        "\n",
        "The process of checking topic assignment is repeated for each word in every document, cycling through the entire collection of documents multiple times. This iterative updating is the key feature of LDA that generates a final solution with coherent topics.\n",
        "\n",
        "Reference: https://towardsdatascience.com/latent-dirichlet-allocation-15800c852699\n"
      ],
      "metadata": {
        "id": "EhOFLfNnlHkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use tf (raw term count) features for LDA."
      ],
      "metadata": {
        "id": "OE7jyOd1g8g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_vectorizer = CountVectorizer(\n",
        "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        ") # max_df = 0.95 means \"ignore terms that appear in more than 95% of the documents\"\n",
        "  # min_df = 2 means \"ignore terms that appear in less than 2 documents\". \n",
        "tf = tf_vectorizer.fit_transform(body_list)"
      ],
      "metadata": {
        "id": "3CM2LpdBg9vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting LDA models"
      ],
      "metadata": {
        "id": "8HBfhfghTb6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_components,\n",
        "    max_iter=5,\n",
        "    learning_method=\"online\", #'online': Online variational Bayes method. \n",
        "    # In each EM update, use mini-batch of training data to update the ``components_``\n",
        "    # variable incrementally. The learning rate is controlled by the\n",
        "    # ``learning_decay`` and the ``learning_offset`` parameters.\n",
        "    learning_offset=50.0, # A (positive) parameter that downweights early iterations in online learning.\n",
        "    random_state=0,\n",
        ").fit(tf)\n",
        "\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
      ],
      "metadata": {
        "id": "5EujacxwTcP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each topic we'll print the 20 most significant words, hence mathematically speaking the words with the highest probability to appear in the topic. This is showing some interesting patterns already: some topics will be related to markets, stocks, options, and everything realted to finance. They have the stock names, like gme, and amc.\n",
        "### Checking the result:\n",
        "We can check the proportion of topics that have been assigned to the a document using the lines of code given below.\n",
        "\n"
      ],
      "metadata": {
        "id": "gEDqfX6YlCoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_body_list = df.original_body.tolist()\n",
        "doc_n = 2179\n",
        "print('\\033[1m' + 'Text: ' + '\\033[0m', original_body_list[doc_n])\n"
      ],
      "metadata": {
        "id": "oTI8eTJopGSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda.fit_transform(tf[2179])"
      ],
      "metadata": {
        "id": "jVCrS1B9k43k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\033[1m' + 'Topic: ' + '\\033[0m')\n",
        "for i,topic in enumerate(lda_output[doc_n]):\n",
        "  print(\"Topic \",i+1,\": \",round(topic*100,3),\"%\")"
      ],
      "metadata": {
        "id": "RQ_XgrSpRptF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 7 \n",
        "Apply Count Vectorization (tf_vectorizer function) to the title_list here. \n"
      ],
      "metadata": {
        "id": "FGRgsuG8uatv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_list = df['clean_title'].tolist() \n",
        "tf_title =  # code\n"
      ],
      "metadata": {
        "id": "pnQ3USehu6LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 8 \n",
        "Apply the LDA to tf and plot the first 10 topic."
      ],
      "metadata": {
        "id": "gQmg-Eg7vYyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_title = LatentDirichletAllocation(\n",
        "    n_components=n_components,\n",
        "    max_iter=5,\n",
        "    learning_method=\"online\",\n",
        "    learning_offset=50.0,\n",
        "    random_state=0,\n",
        ")\n",
        "#### file the code here\n",
        "lda_title.fit( )#code\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(lda_title, tf_feature_names, n_top_words, \"Topics in LDA model\")"
      ],
      "metadata": {
        "id": "-Y7K_u-jvzcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "Finding patterns and understanding the hidden structure of data is a complicated task. Especially when we are dealing with messy and unstructured data as text. Topic models such as LDA or NMF are useful techniques to discover the most prominent topics in such documents. While these results are often very revealing already, it's also possible to use them as a starting point, for example for a labeling exercise for supervised text classification. "
      ],
      "metadata": {
        "id": "36pobMl7oFrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions? \n"
      ],
      "metadata": {
        "id": "de8fzxBEmpPu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HYmIhBkmozd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}